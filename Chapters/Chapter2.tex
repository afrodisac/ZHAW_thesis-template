% Indicate the main file. Must go at the beginning of the file.
% !TEX root = ../main.tex

%----------------------------------------------------------------------------------------
% CHAPTER 2
%----------------------------------------------------------------------------------------
\chapter{Methods} % Main chapter title
\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter1}
In this chapter, the relevant literature and background information is discussed so the reader is more easily able to follow the thesis. 
\section{Cellular Automata}
\subsection{What Are Cellular Automata}
A Cellular Automaton (CA) is a system that typically consists of a discrete lattice or grid of cells.
Each cell can have a discrete state, such as alive or dead, 0 or 1, etc. The cells in the grid are updated
based on simple rules that depend on the cells’ local neighbourhood. The entire system is typically
updated simultaneously. What makes these systems fascinating is that even with very simple rules,
complex emergent behavior can arise.
There are two well-known CA models that I will briefly mention:

\begin{enumerate}
	\item Wolfram’s elementary CA is a one-dimensional CA with a local neighborhood of size 3, which
	includes the cell itself and its right and left neighbors. Some of these rules result in simple
	behavior, while others exhibit incredibly complex behaviors, such as the famous Rule 30 or the
	Turing-complete Rule 110.
	\item John Conway’s Game of Life is perhaps the most famous CA model. It is a two-dimensional
	CA on a square lattice that has been extensively studied, with new discoveries still being made
	to this day. This system uses the Moore’s neighborhood, which is a 3x3 neighborhood that
	includes the central cell. The Game of Life produces incredible emergent behavior and is also
	Turing-complete.
\end{enumerate}

\subsection{Common notation of CA}
[placeholder]
\subsection{Why are they useful?}
In the above section, although beautiful and impressive, the exampels are not practival. But the same motivation and methods can be extrapolated to model physicals systems. Especially many differentiable equations that can be descritized in time and space can be modeled using CA. Many examples have been demostrated over the years. In the following \ref{Chapter3}, the CADDIE2D model is discussed in detail. But other examples include: particle simulation, chemical reactions, fire modeling, human and animal dynaimcs to name a few. [must find references for this.]

\section{Deep Learning}
\subsection{What is deep learning}
\subsection{Optimizers for backpropogation}
\subsection{Common loss functions for regression tasks}
I create this subsection because I think this needs to be carefully considered. For growing images, L2
loss is clearly the best option to reproduce images / learning to grow them. However, depending on
the application of these models the loss function needs to be carefully considered and there are many
to choose from. An example of this is from the paper [3, Self-Organizing Textures] where they use a
L2 loss of gram-matrices by using the raw activations of VGG (Visual Geometry Group Net).
Here we can also look into applying physical constraints to the model, for e.g. adhering to energy
/ mass conservation.
\subsection{Convolutional Neural Network}

A Convolutional Neural Network (CNN) is a type of Artificial Neural Network (ANN) that is commonly used in image and video recognition, natural language processing, and other tasks that involve processing input data with a grid-like structure.

The key characteristic of a CNN is the use of convolutional layers, which apply a set of filters to the input data to extract relevant features for the task at hand. The filters are typically small in size and designed to detect simple patterns, such as edges or corners. The output of the convolutional layer then goes through a non-linear activation function, such as the rectified linear unit (ReLU), to introduce non-linearity into the network.

In addition to convolutional layers, a CNN may also include other types of layers such as pooling layers, which reduce the spatial dimensions of the input data by selecting the maximum or average value from a set of neighboring pixels, and fully connected layers, which connect every neuron in one layer to every neuron in the next layer.

CNNs have achieved state-of-the-art performance on many computer vision tasks, such as image classification, object detection, and segmentation, and are widely used in industry and academia.

\subsection{CNN as a CA}
As it turns out, CNN's and CA's are extremely similar. Cellular automata (CA) and convolutional neural networks (CNN) are both types of mathematical models that can be used to process and analyze data with a grid-like structure, such as images or time series data. \cite{PhysRevE.100.032402}

Both models operate by processing the input data in a local and hierarchical manner. In a CA, the state of each cell is updated based on the states of its neighboring cells, while in a CNN, filters are applied to local patches of the input data to extract relevant features. However, one can think of a neighbourhood as a NxM kernel filled with 1s. In fact, by utilizing a cleverly constructed kernel and activation function, one can model many CA's by performing this convolution and applying an activation function. A simple example of this is game of life as a convolusion: \\ \\
$
\begin{bmatrix}
	1 & 1 & 1 \\
	1 & 9 & 1 \\
	1 & 1 & 1
\end{bmatrix}
$
Kernel to be applied \\ \\
And activation function (or local update rule): \\ \\
$
f(convolution) = \begin{cases}
	1, & \text{if } convolution = 3 \text{ or } convolution = 11 \text{ or } convolution = 12 \\
	0, & \text{otherwise}
\end{cases}
$ \\ \\

It turns out that CNN's can learn the rules of arbitrary CA's, for example game of life, and it doesn't need a particularly deep architecture to do so. it does this by essentially learning the kernel / filter weights that needs to be applied. To use a CNN as a CA, you essentially turn the CA into a binary classification problem to predict the state of each cell.

\subsection{Neural Cellular Automata}

Now that we have some basic foundational knowledge we can move on to the real NCA (or differentiable, self-organizing systems). As discussed in the section above, a CNN can be seen as a type of CA, but utilizing a continuous state-space instead of a discrete one (as well as an arbitrary number of hidden states / channels). As described in the growing NCA paper \cite{growing_nca}, this model can be thought of as a “Recurrent Residual Convolutional Network with ‘per-pixel’ Dropout”. The 'per-pixel dropout' refers to the stochastic updating of cells. We start from a single black pixel in the center of a blank image and run the model on it for a certain number of steps where the output of the model becomes the new input. Then compare the final result of this model with the target image we are training the model. We then perform a per-pixel difference loss function (L2 loss). Then based on this loss we use ADAM optimizer to back-propagate through time to adjust the weights of the model until the model learns to 'grow' the target image from a seed state.a seed state.
